{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LibraJiang\\AppData\\Local\\Temp\\ipykernel_28580\\1557211701.py:1: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\c10/core/TensorImpl.h:1924.)\n",
      "  _ = torch.tensor([0.2126, 0.7152, 0.0722], names=['c'])\n"
     ]
    }
   ],
   "source": [
    "_ = torch.tensor([0.2126, 0.7152, 0.0722], names=[\"c\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = torch.randn(3, 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_t = torch.randn(2, 3, 5, 5)  # shape [batch, channels, rows, columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_naive = img_t.mean(-3)\n",
    "img_gray_naive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_gray_naive = batch_t.mean(-3)\n",
    "batch_gray_naive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tensor([0.2126, 0.7152, 0.0722])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1, 1]),\n",
       " tensor([[[0.2126]],\n",
       " \n",
       "         [[0.7152]],\n",
       " \n",
       "         [[0.0722]]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsqueezed_weights = weights.unsqueeze(-1).unsqueeze_(-1)\n",
    "unsqueezed_weights.shape, unsqueezed_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_img = img_t * unsqueezed_weights\n",
    "weighted_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5, 5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_batch = batch_t * unsqueezed_weights\n",
    "weighted_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gray_weighted = weighted_img.sum(-3)\n",
    "batch_gray_weighted = weighted_batch.sum(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted.shape, batch_gray_weighted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5, 5]), torch.Size([3]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t.shape, weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"爱因斯坦求和约定\"，爱因斯坦在研究相对论时，曾经对冗余的求和公式做了一个简化版的约定。AI中的具体实现一般指np和torch中的einsum()。\n",
    "不推荐使用，看起来较为直观，但更容易出错，且不易debug。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "einsum()的四句口诀：\n",
    "- 外部重复做乘积 ,\n",
    "\n",
    "- 内部重复把数取 ,\n",
    "\n",
    "- 从有到无要求和 ,\n",
    "\n",
    "- 重复默认要丢弃."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted_fancy = torch.einsum(\"...chw,c->...hw\", img_t, weights)\n",
    "img_gray_weighted_fancy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 5])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_gray_weighted_fancy = torch.einsum(\"...chw,c->...hw\", batch_t, weights)\n",
    "batch_gray_weighted_fancy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2126, 0.7152, 0.0722], names=('channels',))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names=[\"channels\"])\n",
    "weights_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_named = img_t.refine_names(..., \"channels\", \"rows\", \"colums\")\n",
    "batch_named = batch_t.refine_names(..., \"channels\", \"rows\", \"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8458, -1.5019,  2.1762, -1.8889,  0.3466],\n",
       "         [ 1.1400, -0.3311,  0.0593, -0.5156, -0.2877],\n",
       "         [ 0.0842,  1.4210, -0.6301,  2.1071,  0.2736],\n",
       "         [-0.0924,  2.3741, -0.1490,  0.9867, -1.4424],\n",
       "         [-0.3587,  0.3222,  0.2832, -1.3613, -0.8284]],\n",
       "\n",
       "        [[-3.5773, -0.0695, -1.0679, -0.3833,  0.3665],\n",
       "         [-0.1612,  0.7765,  1.4100,  2.0078, -1.9607],\n",
       "         [-0.8928,  2.1080, -0.3435,  0.4030,  1.5907],\n",
       "         [ 0.4806,  1.6486, -0.1582,  0.4909, -0.8611],\n",
       "         [-0.8707, -0.5649,  0.4726,  0.7306, -3.1533]],\n",
       "\n",
       "        [[-1.1492, -1.0514, -1.1450, -0.0886, -0.1075],\n",
       "         [-0.9897, -0.7020,  1.3134, -1.4369,  0.1279],\n",
       "         [-0.4654,  0.8840,  0.9332, -0.1327, -0.7569],\n",
       "         [ 1.1022, -1.2850,  0.7903, -0.2787, -0.7794],\n",
       "         [-0.6142,  0.4623,  2.1319,  0.1869,  0.7083]]],\n",
       "       names=('channels', 'rows', 'colums'))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.5502,  1.0179, -0.9398,  1.2022,  1.4469],\n",
       "          [-0.4110, -2.1574, -0.0588,  0.9639,  1.0047],\n",
       "          [ 0.0604,  1.7600, -1.2831,  0.3098,  1.2640],\n",
       "          [-1.1063,  0.0469,  1.0534,  0.5009, -0.5312],\n",
       "          [ 0.0398, -0.9085,  0.0977,  0.4397,  0.6624]],\n",
       "\n",
       "         [[ 0.3348, -0.2372, -1.0977,  0.3140, -0.5675],\n",
       "          [ 0.7674, -0.7300,  0.5761,  0.6203,  0.2813],\n",
       "          [ 0.4190,  0.3505,  0.0930, -0.3457, -1.4747],\n",
       "          [-0.6163, -0.7263,  1.8851, -0.3143, -0.0369],\n",
       "          [-1.2528, -0.9152,  0.8228,  0.4309, -0.7692]],\n",
       "\n",
       "         [[-0.8637,  2.5586, -1.8656, -1.0413, -0.8514],\n",
       "          [-1.5720,  0.4588,  1.2960,  0.7910,  1.6022],\n",
       "          [ 0.8303, -0.6765, -0.4364, -1.5555,  0.5091],\n",
       "          [ 1.1311, -0.6482, -1.4978,  0.5350, -0.8970],\n",
       "          [-0.3692, -0.4460,  0.4988, -0.9703,  1.6142]]],\n",
       "\n",
       "\n",
       "        [[[ 0.7030,  0.1718, -1.0258, -0.6740, -0.2337],\n",
       "          [-0.2686,  0.5959, -0.5900, -0.1290,  0.3990],\n",
       "          [-0.3384, -0.6687,  0.1449, -1.2938, -0.2096],\n",
       "          [ 0.8643, -1.2172,  3.0115,  1.6597, -1.7562],\n",
       "          [-0.4843, -0.6200, -3.1283,  0.6124, -1.0212]],\n",
       "\n",
       "         [[-0.3533, -0.3774, -0.2771,  0.2324, -0.9449],\n",
       "          [ 2.2278,  0.7726, -0.2927, -0.2534,  1.2743],\n",
       "          [-1.2872,  0.9975, -0.3636,  0.1995,  0.4215],\n",
       "          [ 0.0588,  0.8027,  0.8410, -1.4408, -0.3528],\n",
       "          [-0.0727, -0.2280,  0.0035, -1.9455,  1.1479]],\n",
       "\n",
       "         [[-0.0128,  0.3329, -0.2173,  1.1180, -0.1580],\n",
       "          [-0.6067,  0.7514, -1.1686,  2.0909, -0.0074],\n",
       "          [ 1.1029, -0.3786,  1.3673, -0.5555,  0.0590],\n",
       "          [ 0.1242, -0.6920,  1.2638, -0.4443,  0.2871],\n",
       "          [-0.4740, -1.5782, -0.7598,  1.3060,  1.1135]]]],\n",
       "       names=(None, 'channels', 'rows', 'columns'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 5])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = img_t\n",
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 5])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[None]\n",
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8458, -1.5019,  2.1762, -1.8889,  0.3466],\n",
       "         [ 1.1400, -0.3311,  0.0593, -0.5156, -0.2877],\n",
       "         [ 0.0842,  1.4210, -0.6301,  2.1071,  0.2736],\n",
       "         [-0.0924,  2.3741, -0.1490,  0.9867, -1.4424],\n",
       "         [-0.3587,  0.3222,  0.2832, -1.3613, -0.8284]],\n",
       "\n",
       "        [[-3.5773, -0.0695, -1.0679, -0.3833,  0.3665],\n",
       "         [-0.1612,  0.7765,  1.4100,  2.0078, -1.9607],\n",
       "         [-0.8928,  2.1080, -0.3435,  0.4030,  1.5907],\n",
       "         [ 0.4806,  1.6486, -0.1582,  0.4909, -0.8611],\n",
       "         [-0.8707, -0.5649,  0.4726,  0.7306, -3.1533]],\n",
       "\n",
       "        [[-1.1492, -1.0514, -1.1450, -0.0886, -0.1075],\n",
       "         [-0.9897, -0.7020,  1.3134, -1.4369,  0.1279],\n",
       "         [-0.4654,  0.8840,  0.9332, -0.1327, -0.7569],\n",
       "         [ 1.1022, -1.2850,  0.7903, -0.2787, -0.7794],\n",
       "         [-0.6142,  0.4623,  2.1319,  0.1869,  0.7083]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.0000, -1.5019,  2.1762, -1.8889,  0.3466],\n",
       "          [ 1.1400, -0.3311,  0.0593, -0.5156, -0.2877],\n",
       "          [ 0.0842,  1.4210, -0.6301,  2.1071,  0.2736],\n",
       "          [-0.0924,  2.3741, -0.1490,  0.9867, -1.4424],\n",
       "          [-0.3587,  0.3222,  0.2832, -1.3613, -0.8284]],\n",
       " \n",
       "         [[-3.5773, -0.0695, -1.0679, -0.3833,  0.3665],\n",
       "          [-0.1612,  0.7765,  1.4100,  2.0078, -1.9607],\n",
       "          [-0.8928,  2.1080, -0.3435,  0.4030,  1.5907],\n",
       "          [ 0.4806,  1.6486, -0.1582,  0.4909, -0.8611],\n",
       "          [-0.8707, -0.5649,  0.4726,  0.7306, -3.1533]],\n",
       " \n",
       "         [[-1.1492, -1.0514, -1.1450, -0.0886, -0.1075],\n",
       "          [-0.9897, -0.7020,  1.3134, -1.4369,  0.1279],\n",
       "          [-0.4654,  0.8840,  0.9332, -0.1327, -0.7569],\n",
       "          [ 1.1022, -1.2850,  0.7903, -0.2787, -0.7794],\n",
       "          [-0.6142,  0.4623,  2.1319,  0.1869,  0.7083]]]),\n",
       " tensor([[[ 1.0000, -1.5019,  2.1762, -1.8889,  0.3466],\n",
       "          [ 1.1400, -0.3311,  0.0593, -0.5156, -0.2877],\n",
       "          [ 0.0842,  1.4210, -0.6301,  2.1071,  0.2736],\n",
       "          [-0.0924,  2.3741, -0.1490,  0.9867, -1.4424],\n",
       "          [-0.3587,  0.3222,  0.2832, -1.3613, -0.8284]],\n",
       " \n",
       "         [[-3.5773, -0.0695, -1.0679, -0.3833,  0.3665],\n",
       "          [-0.1612,  0.7765,  1.4100,  2.0078, -1.9607],\n",
       "          [-0.8928,  2.1080, -0.3435,  0.4030,  1.5907],\n",
       "          [ 0.4806,  1.6486, -0.1582,  0.4909, -0.8611],\n",
       "          [-0.8707, -0.5649,  0.4726,  0.7306, -3.1533]],\n",
       " \n",
       "         [[-1.1492, -1.0514, -1.1450, -0.0886, -0.1075],\n",
       "          [-0.9897, -0.7020,  1.3134, -1.4369,  0.1279],\n",
       "          [-0.4654,  0.8840,  0.9332, -0.1327, -0.7569],\n",
       "          [ 1.1022, -1.2850,  0.7903, -0.2787, -0.7794],\n",
       "          [-0.6142,  0.4623,  2.1319,  0.1869,  0.7083]]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[0, 0, 0] = 1\n",
    "temp, img_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None作为占位符，可以用来扩展tensor的维度，直接使用`tensor = tensor[None]`将在高维增加一维，使用`tensor = tensor[..., None]`将在低维增加一维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 2])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points = points[None]\n",
    "points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 2, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = points[..., None]\n",
    "points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_aligned = weights_named.align_as(img_named)\n",
    "weights_aligned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 5])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1.]]]),\n",
       " tensor([1, 2, 3], names=('channels',)))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.ones_like(img_t)\n",
    "test_weights = torch.tensor([1, 2, 3], names=[\"channels\"])\n",
    "test, test_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5, 5]), torch.Size([3, 1, 1]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = test.refine_names(..., \"channels\", \"rows\", \"columns\")\n",
    "test_weights = test_weights.align_as(test)\n",
    "test.shape, test_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.]],\n",
       "\n",
       "        [[3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.]]], names=('channels', 'rows', 'columns'))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test * test_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当形状为[3,4,5]的张量与形状为[3,1,1]的张量相乘时，计算逻辑如下：\n",
    "\n",
    "1. 首先，将形状为[3,1,1]的张量进行广播，使其形状与[3,4,5]的张量相匹配。广播的规则是在缺失的维度上将维度大小为1的张量进行复制，直到两个张量的形状相同。\n",
    "\n",
    "2. 然后，对应位置的元素进行相乘。由于广播后两个张量的形状相同，所以可以直接对应位置的元素相乘。\n",
    "\n",
    "3. 最后，得到的结果是一个形状为[3,4,5]的张量，其中每个元素是原始张量对应位置元素的乘积。\n",
    "\n",
    "需要注意的是，广播操作只会复制元素的值，并不会增加张量的内存占用。因此，计算逻辑是在不增加内存占用的情况下对应位置的元素相乘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2126, -0.3193,  0.4627, -0.4016,  0.0737],\n",
       "         [ 0.2424, -0.0704,  0.0126, -0.1096, -0.0612],\n",
       "         [ 0.0179,  0.3021, -0.1340,  0.4480,  0.0582],\n",
       "         [-0.0197,  0.5047, -0.0317,  0.2098, -0.3067],\n",
       "         [-0.0763,  0.0685,  0.0602, -0.2894, -0.1761]],\n",
       "\n",
       "        [[-2.5585, -0.0497, -0.7638, -0.2741,  0.2621],\n",
       "         [-0.1153,  0.5554,  1.0084,  1.4360, -1.4023],\n",
       "         [-0.6385,  1.5077, -0.2457,  0.2882,  1.1377],\n",
       "         [ 0.3438,  1.1791, -0.1131,  0.3511, -0.6158],\n",
       "         [-0.6227, -0.4040,  0.3380,  0.5225, -2.2552]],\n",
       "\n",
       "        [[-0.0830, -0.0759, -0.0827, -0.0064, -0.0078],\n",
       "         [-0.0715, -0.0507,  0.0948, -0.1037,  0.0092],\n",
       "         [-0.0336,  0.0638,  0.0674, -0.0096, -0.0546],\n",
       "         [ 0.0796, -0.0928,  0.0571, -0.0201, -0.0563],\n",
       "         [-0.0443,  0.0334,  0.1539,  0.0135,  0.0511]]],\n",
       "       names=('channels', 'rows', 'colums'))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_named * weights_aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的img_named[...,:3]是为了在与weights_named相乘时让后者可以广播，也就是让二者满足相乘的条件。\n",
    "但是可以看到，二者依旧无法相乘，因为二者的最低维命名不匹配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when attempting to broadcast dims ['channels', 'rows', 'colums'] and dims ['channels']: dim 'colums' and dim 'channels' are at the same position from the right but do not match.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    gray_named = (img_named[..., :3] * weights_named).sum(\"channels\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch中的*等价于torch.mul()，本质上是element-wise的乘法，也就是逐元素进行乘法，尺寸不统一的先根据广播机制扩展至相同尺寸，再进行对应元素的乘法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4,  9, 16, 25])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([2, 3, 4, 5])\n",
    "b = torch.tensor([2, 3, 4, 5])\n",
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), ('rows', 'colums'))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray_named = (img_named * weights_aligned).sum(\"channels\")\n",
    "gray_named.shape, gray_named.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), (None, None))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray_plain = gray_named.rename(None)\n",
    "gray_plain.shape, gray_plain.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlwpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
