{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[[1, 2, 3], [2, 2, 2]], [[1, 2, 3], [2, 2, 2]]])\n",
    "n_1 = np.sum(a, axis=0)\n",
    "n_2 = np.sum(a, axis=1)\n",
    "n_3 = np.sum(a, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2, 4, 6],\n",
       "        [4, 4, 4]]),\n",
       " (2, 3))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_1, n_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[3, 4, 5],\n",
       "        [3, 4, 5]]),\n",
       " (2, 3))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_2, n_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[6, 6],\n",
       "        [6, 6]]),\n",
       " (2, 2))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_3, n_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.from_numpy(a).to(dtype=torch.float32)\n",
    "n1 = torch.mean(b, dim=0)\n",
    "n2 = torch.mean(b, dim=1)\n",
    "n3 = torch.mean(b, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3.],\n",
       "         [2., 2., 2.]]),\n",
       " tensor([[1.5000, 2.0000, 2.5000],\n",
       "         [1.5000, 2.0000, 2.5000]]),\n",
       " tensor([[2., 2.],\n",
       "         [2., 2.]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1, n2, n3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6464, 0.0657, 0.0173],\n",
      "         [0.8182, 0.7192, 0.7511]],\n",
      "\n",
      "        [[0.7762, 0.7896, 0.7568],\n",
      "         [0.8208, 0.2054, 0.1491]]])\n",
      "dim=0的结果是：\n",
      " tensor([[[0.4676, 0.3265, 0.3231],\n",
      "         [0.4994, 0.6257, 0.6461]],\n",
      "\n",
      "        [[0.5324, 0.6735, 0.6769],\n",
      "         [0.5006, 0.3743, 0.3539]]]) \n",
      "\n",
      "dim=1的结果是：\n",
      " tensor([[[0.4572, 0.3422, 0.3244],\n",
      "         [0.5428, 0.6578, 0.6756]],\n",
      "\n",
      "        [[0.4889, 0.6420, 0.6474],\n",
      "         [0.5111, 0.3580, 0.3526]]]) \n",
      "\n",
      "dim=2的结果是：\n",
      " tensor([[[0.4779, 0.2674, 0.2548],\n",
      "         [0.3520, 0.3188, 0.3292]],\n",
      "\n",
      "        [[0.3340, 0.3385, 0.3275],\n",
      "         [0.4875, 0.2635, 0.2490]]]) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2, 3]), torch.Size([2, 2, 3]), torch.Size([2, 2, 3]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(size=[2, 2, 3])\n",
    "print(y)\n",
    "# y的size是2,2,3。可以看成有两张表，每张表2行3列\n",
    "net_1 = nn.Softmax(dim=0)\n",
    "net_2 = nn.Softmax(dim=1)\n",
    "net_3 = nn.Softmax(dim=2)\n",
    "n1 = net_1(y)\n",
    "n2 = net_2(y)\n",
    "n3 = net_3(y)\n",
    "print(\"dim=0的结果是：\\n\", n1, \"\\n\")\n",
    "print(\"dim=1的结果是：\\n\", n2, \"\\n\")\n",
    "print(\"dim=2的结果是：\\n\", n3, \"\\n\")\n",
    "n1.shape, n2.shape, n3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结：\n",
    "不论是np中的axis还是torch中的dim，在运算中作为参数，都表示该运算沿着指定的\"轴\"，也就是维度进行运算，至于在运算后是否会降维，则只与运算本身的规则有关，比如sum()和mean()等运算，本身就是`List -> Num`的运算，运行后自然指定的dim会被消去；而Softmax()这种`List -> List`的运算，则会保留原有维度。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlwpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
